import webrtcvad
import pyaudio
import collections
import numpy as np
from funasr import AutoModel
import torch
import os
import psutil
from typing import List, Dict, Any
import time
from datetime import datetime

from speech.tools.VoiceprintRegistration import VoiceprintRegistration


class Procedure(object):
    def __init__(self, model):
        hotword_file = "./hotwords.txt"
        self.model = model
        self.hotwords = ''
        if hotword_file is not None and os.path.exists(hotword_file):
            with open(hotword_file, "r", encoding="utf-8") as f:
                lines = f.readlines()
                lines = [line.strip() for line in lines]
            self.hotwords = " ".join(lines)

        print("æ¨¡å‹åŠ è½½å®Œæˆã€‚")

    def record_until_silence(self, rate=16000, chunk_ms=30, padding_ms=300):
        """
        rateï¼šé‡‡æ ·ç‡
        chunk_msï¼šé‡‡éŸ³æ—¶é—´æ®µ
        padding_msï¼šåœæ­¢å‘è¨€æ£€æµ‹æ—¶é—´
        ä»éº¦å…‹é£å½•éŸ³ï¼Œç›´åˆ°æ£€æµ‹åˆ°ä¸€å¥è¯ç»“æŸï¼Œè¿”å›ï¼ˆaudio_bytes,tï¼‰audio_bytesæ˜¯å½•åˆ¶æ—¶æ®µå†…å£°éŸ³å†…å®¹å­—ç¬¦ä¸²ï¼Œtä¸ºè¯­éŸ³å¼€å§‹çš„æ—¶é—´æˆ³ã€‚
        """
        vad = webrtcvad.Vad(3)  # VADçš„æ•æ„Ÿåº¦ï¼Œå¯ä»¥æ˜¯0, 1, 2, 3ã€‚3ä¸ºæœ€é«˜ã€‚
        chunk_size = int(rate * chunk_ms / 1000)
        num_padding_chunks = int(padding_ms / chunk_ms)
        ring_buffer = collections.deque(maxlen=num_padding_chunks)

        pa = pyaudio.PyAudio()
        stream = pa.open(format=pyaudio.paInt16, channels=1, rate=rate,
                         input=True, frames_per_buffer=chunk_size)

        print("...å¼€å§‹å½•éŸ³...")
        voiced_frames = []
        triggered = False
        timestamp1 = int(time.time())
        while True:
            frame = stream.read(chunk_size, exception_on_overflow=False)
            is_speech = vad.is_speech(frame, rate)

            if not triggered:
                ring_buffer.append((frame, is_speech))
                num_voiced = len([f for f, speech in ring_buffer if speech])
                if num_voiced > 0.9 * ring_buffer.maxlen:
                    print("æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹ï¼")
                    triggered = True
                    voiced_frames.extend([f for f, s in ring_buffer])
                    ring_buffer.clear()
            else:
                voiced_frames.append(frame)
                ring_buffer.append((frame, is_speech))
                num_unvoiced = len([f for f, speech in ring_buffer if not speech])
                # if num_unvoiced > 0.9 * ring_buffer.maxlen:
                if num_unvoiced > 0.9 * ring_buffer.maxlen:
                    print("æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸï¼")
                    break
        # timestamp2 = int(time.time())
        print("...å½•éŸ³ç»“æŸ...")
        stream.stop_stream()
        stream.close()
        pa.terminate()
        #
        # dt = datetime.fromtimestamp(timestamp1)
        # formatted_time1 = dt.strftime("%Y-%m-%d %H:%M:%S")
        # dt = datetime.fromtimestamp(timestamp2)
        # formatted_time2 = dt.strftime("%Y-%m-%d %H:%M:%S")
        return b"".join(voiced_frames), timestamp1  # formatted_time1,formatted_time2

    def record_duration_time(self, rate=16000, chunk_ms=30, duration_sec=5):
        """
        è°ƒç”¨å‡½æ•°åˆ™éº¦å…‹é£å¼€å§‹å½•éŸ³ï¼ŒæŒç»­æŒ‡å®šæ—¶é•¿åè‡ªåŠ¨åœæ­¢ï¼Œè¿”å›ï¼ˆaudio_bytes,tï¼‰audio_bytesæ˜¯duration_secæ—¶æ®µå†…å£°éŸ³å†…å®¹å­—ç¬¦ä¸²ï¼Œtä¸ºè¯­éŸ³å¼€å§‹çš„æ—¶é—´æˆ³ã€‚
        :param rate: é‡‡æ ·ç‡(é»˜è®¤16000Hz)
        :param chunk_ms: æ¯æ¬¡è¯»å–çš„éŸ³é¢‘å—æ—¶é•¿(é»˜è®¤30ms)
        :param duration_sec: å½•éŸ³æ—¶é•¿(ç§’ï¼Œé»˜è®¤5ç§’)
        :return: (PCMéŸ³é¢‘æ•°æ®, å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´)
        """
        chunk_size = int(rate * chunk_ms / 1000)
        total_chunks = int(duration_sec * rate / chunk_size)

        pa = pyaudio.PyAudio()
        stream = pa.open(format=pyaudio.paInt16, channels=1, rate=rate,
                         input=True, frames_per_buffer=chunk_size)

        print(f"...å¼€å§‹å½•éŸ³ï¼Œå°†æŒç»­{duration_sec}ç§’...")
        frames = []
        timestamp1 = int(time.time())

        for i in range(total_chunks):
            frame = stream.read(chunk_size, exception_on_overflow=False)
            frames.append(frame)

        # timestamp2 = int(time.time())
        print("...å½•éŸ³ç»“æŸ...")
        stream.stop_stream()
        stream.close()
        pa.terminate()
        print(frames)
        # dt = datetime.fromtimestamp(timestamp1)
        # formatted_time1 = dt.strftime("%Y-%m-%d %H:%M:%S")
        # dt = datetime.fromtimestamp(timestamp2)
        # formatted_time2 = dt.strftime("%Y-%m-%d %H:%M:%S")
        return b"".join(frames), timestamp1  # formatted_time1, formatted_time2

    def get_speech_segments_with_embeddings(self, audio_bytes: bytes, time_start, reg_system, **kwargs) -> List[
        Dict[str, Any]]:
        """
        å¤„ç†éŸ³é¢‘å­—èŠ‚æµï¼Œè¿”å›æ¯ä¸ªè¯­éŸ³ç‰‡æ®µçš„æ–‡æœ¬å’Œå£°çº¹åµŒå…¥ã€‚
        """
        model = self.model
        if not model.vad_model:
            raise ValueError("AutoModelå®ä¾‹å¿…é¡»åœ¨åˆå§‹åŒ–æ—¶åŒ…å«VADæ¨¡å‹ (vad_model)ã€‚")
        if not model.spk_model:
            raise ValueError("AutoModelå®ä¾‹å¿…é¡»åœ¨åˆå§‹åŒ–æ—¶åŒ…å«å£°çº¹æ¨¡å‹ (spk_model)ã€‚")

        print("å¼€å§‹æ¨ç†å¤„ç†...")

        print("1. æ­£åœ¨è¿›è¡ŒVADåˆ‡åˆ†...")
        vad_result = model.inference(
            input=audio_bytes,
            model=model.vad_model,
            kwargs=model.vad_kwargs,
            **kwargs
        )
        vad_segments = vad_result[0]['value']
        print("vad_result", vad_result)
        print(f"VADæ£€æµ‹åˆ° {len(vad_segments)} ä¸ªç‰‡æ®µã€‚")

        if not vad_segments:
            print("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³ç‰‡æ®µã€‚")
            return []

        speech_array = np.frombuffer(audio_bytes, dtype=np.int16)
        sample_rate = 16000
        processed_segments = []

        for i, (start_ms, end_ms) in enumerate(vad_segments):
            print(f"\n--- æ­£åœ¨å¤„ç†ç‰‡æ®µ {i + 1}/{len(vad_segments)} ({start_ms}ms -> {end_ms}ms) ---")
            start_sample = int(start_ms * sample_rate / 1000)
            end_sample = int(end_ms * sample_rate / 1000)
            segment_audio_int16 = speech_array[start_sample:end_sample]

            if len(segment_audio_int16) == 0:
                print("ç‰‡æ®µä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                continue
            print("   a. æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ« (ASR)...")
            segment_audio = (segment_audio_int16 / 32768.0).astype(np.float32)
            asr_result = model.inference(
                input=segment_audio,
                model=model.model,
                hotword=self.hotwords,  # ä¼ å…¥çƒ­è¯
                **kwargs
            )
            segment_text = asr_result[0]['text']

            print("   b. æ­£åœ¨æå–å£°çº¹ (SPK)...")
            spk_result = model.inference(
                input=segment_audio,
                model=model.spk_model,
                kwargs=model.spk_kwargs,
                **kwargs
            )
            segment_embedding = spk_result[0]['spk_embedding']

            # processed_segments.append({
            #     "text": segment_text,
            #     "embedding": segment_embedding,
            #     "time": [datetime.fromtimestamp(time_start+start_ms//1000.0).strftime("%Y-%m-%d %H:%M:%S"),datetime.fromtimestamp(time_start+end_ms//1000.0).strftime("%Y-%m-%d %H:%M:%S")]
            # })
            #  ç”¨æ³¨å†Œç³»ç»Ÿè¯†åˆ«è¯´è¯äºº
            speaker = reg_system.embedding_to_speaker(segment_embedding)

            # æ”¹æˆ HH:MM:SS
            time_start = time.time()  # å½“å‰æ—¶é—´çš„ Unix æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
            start_time_str = datetime.fromtimestamp(time_start + start_ms / 1000.0).strftime("%H:%M:%S")
            end_time_str = datetime.fromtimestamp(time_start + end_ms / 1000.0).strftime("%H:%M:%S")

            processed_segments.append({
                "text": self.add_punctuation(segment_text),
                "speaker": speaker,  # çœŸå®çš„è¯´è¯äºº
                "time": [start_time_str, end_time_str]
            })

        print("\næ‰€æœ‰ç‰‡æ®µå¤„ç†å®Œæ¯•ã€‚")
        return processed_segments

    def add_punctuation(self, text: str):
        """
        è°ƒç”¨ punc_model ç»™æ–‡æœ¬åŠ æ ‡ç‚¹
        """
        if self.model.punc_model is None:
            print("æœªåŠ è½½æ ‡ç‚¹æ¨¡å‹(punc_model)ï¼Œè¯·åœ¨AutoModelåˆå§‹åŒ–æ—¶ä¼ å…¥ã€‚")
            return text

        # ğŸ” æ‰“å°è¾“å…¥ï¼Œç¡®è®¤æ˜¯ä¸æ˜¯ Noneã€ç©ºä¸²æˆ–ç±»å‹é”™è¯¯
        print(f"[PUNC] è¾“å…¥æ–‡æœ¬ç±»å‹: {type(text)}, å†…å®¹: '{text}'")

        try:
            res = self.model.inference(
                text,
                model=self.model.punc_model,
                kwargs=self.model.punc_kwargs
            )
            print(f"[PUNC] æ¨¡å‹è¾“å‡º: {res}")
        except Exception as e:
            print(f"[PUNC] æ ‡ç‚¹æ¨¡å‹æ¨ç†æŠ¥é”™: {e}")
            return text  # å‡ºé”™æ—¶ç›´æ¥è¿”å›åŸå§‹æ–‡æœ¬

        if len(res) > 0 and "text" in res[0]:
            return res[0]["text"]
        else:
            return text


if __name__ == '__main__':
    home_directory = os.path.expanduser("~")
    asr_model_path = os.path.join(home_directory, ".cache", "modelscope", "hub", "models", "iic",
                                  "speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch")
    asr_model_revision = "v2.0.4"
    vad_model_path = os.path.join(home_directory, ".cache", "modelscope", "hub", "models", "iic",
                                  "speech_fsmn_vad_zh-cn-16k-common-pytorch")
    vad_model_revision = "v2.0.4"
    punc_model_path = os.path.join(home_directory, ".cache", "modelscope", "hub", "models", "iic",
                                   "punc_ct-transformer_zh-cn-common-vocab272727-pytorch")
    punc_model_revision = "v2.0.4"
    spk_model_path = os.path.join(home_directory, ".cache", "modelscope", "hub", "models", "iic",
                                  "speech_campplus_sv_zh-cn_16k-common")
    spk_model_revision = "v2.0.4"
    hotword_file = "./hotwords.txt"
    ngpu = 1
    device = "cuda" if torch.cuda.is_available() else "cpu"
    ncpu = psutil.cpu_count()

    # ASR æ¨¡å‹
    model = AutoModel(model=asr_model_path,
                      model_revision=asr_model_revision,
                      vad_model=vad_model_path,
                      vad_model_revision=vad_model_revision,
                      punc_model=punc_model_path,
                      punc_model_revision=punc_model_revision,
                      spk_model=spk_model_path,
                      spk_model_revision=spk_model_revision,
                      ngpu=ngpu,
                      ncpu=ncpu,
                      device=device,
                      disable_pbar=True,
                      disable_log=True,
                      disable_update=True
                      )
    try:
        # 1. åˆå§‹åŒ–å¤„ç†ç±»
        procedure = Procedure(model=model)
        #
        # # 2. è¿›å…¥ä¸»å¾ªç¯
        # while True:
        # 3. ç­‰å¾…ç”¨æˆ·æŒ‡ä»¤

        # 4. ä»éº¦å…‹é£å½•åˆ¶ä¸€å¥è¯,è¿”å›æ—¶é—´æˆ³
        audio_data_bytes, t1 = procedure.record_duration_time()

        registration_system = VoiceprintRegistration(model=model)

        # 5. è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
        results = procedure.get_speech_segments_with_embeddings(audio_data_bytes, t1,registration_system)

        print(results)
        sentence = procedure.add_punctuation(
            "æˆ‘ çš„ å å­— å« åš ä½• æ™¨ å…‰ å¦ å…‹ æ˜¯ æ²¡ æœ‰ å è§† é•œ çš„ é»‘ å“¥ ä»¬ çš„ è¯­ è¨€ æ˜¯ ä¸ åŒ çš„")
        print(sentence)

        # 6. æ‰“å°ç»“æœ
        print("\n\n" + "=" * 20 + " æ¨ç†ç»“æœ " + "=" * 20)
        # if results:
        #     for i, res in enumerate(results):
        #         print(f"ç‰‡æ®µ {i + 1}:")
        #         print(f"  è¯†åˆ«æ–‡æœ¬: '{res['text']}'")
        #         print(f"  å£°çº¹åµŒå…¥å½¢çŠ¶: {res['embedding'].shape}")
        #         print(f"  å£°çº¹åµŒå…¥è®¾å¤‡: {res['embedding'].device}")
        #         # æ‰“å°å‰5ä¸ªç»´åº¦çš„å€¼ï¼Œä½œä¸ºç¤ºä¾‹
        #         print(f"  å£°çº¹åµŒå…¥é¢„è§ˆ: {res['embedding'].flatten()[:5].tolist()}...")
        #         print(f"  è¯­éŸ³æ—¶é—´,{res['time']}...")
        #         print("-" * 40)
        # else:
        #     print("æ²¡æœ‰è¯†åˆ«åˆ°æœ‰æ•ˆç»“æœã€‚")
        print(results)
        print("=" * 52 + "\n")

    except KeyboardInterrupt:
        print("\nç¨‹åºå·²ç»ˆæ­¢ã€‚")
    except Exception as e:
        print(f"\nå‘ç”Ÿé”™è¯¯: {e}")
